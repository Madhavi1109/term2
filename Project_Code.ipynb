<img src="https://github.com/insaid2018/Term-1/blob/master/Images/INSAID_Full%20Logo.png?raw=true" width="240" height="360" /><br>
                            __Exploratory Data Analysis__

## Table of Content

1. [Problem Statement](#section1)<br>
    -1.1 [INTRODUCTION](#section101)<br>
    -1.2 [Data Source and Dataset](#section102)<br>
2. [Load the Packages and Data](#section2)<br>
3. [Data Profiling](#section3)<br>
    -3.1 [Is there a relationship between sugarpercent and winpercent](#section301)<br>
    -3.2 [Is there a relationship between pricepercent and winpercent](#section302)<br>
    -3.3 [Most popular and least popular candy](#section303)<br>
4. [Model Training](#section4)<br>

## 1. PROBLEM STATEMENT

The dataset contains study of candy data.In this project,we will analyse as such
     Which is the best popular and least popular candy.<br>
     What is the flavour of the least and best popular candy.<br>
     Predictive modeling

![candy.png](attachment:candy.png)

<a id=section101></a>
### 1.1 INTRODUCTION
Exploratory Data Analysis (EDA) is the series of asking questions and applying statistics and visualization techniques to answer those questions and to uncover the hidden insights from the data.A case study on the candy data set is done to explore the most common EDA techniques.

<a id=section102></a>
### 1.2 Data Source and DataSet

__a__ How was it collected

-__Name__: Candy Data
<br>
-__Description__: The data set contains attributes like the different candies,the flavour of the candy if it is chocolate or fruity or caramel or peanutmondy and so on and its corresponding sugar percentage, winpercentage and price percentage.
<br>
-__Is it a Sample data__: Yes, it is a sample set of 87 rows.
    


## 2. Load the Packages and Data

#import the libraries
import numpy as np                                                               #linear algebra
import pandas as pd                                                              #data processing and accessing i/o csv files
import pandas_profiling as pp
import matplotlib.pyplot as plt
from matplotlib.pyplot import pie, axis, show
%matplotlib inline 
from sklearn import metrics
import seaborn as sns
sns.set()
import warnings                                                                 # Ignore warning related to pandas_profiling
warnings.filterwarnings('ignore')
import math


#import the candy data file under the dir https://github.com/insaid2018/Term-2/blob/master/Projects/candy-data.csv

source_data = pd.read_csv(r"https://raw.githubusercontent.com/insaid2018/Term-2/master/Projects/candy-data.csv") 

source_data.head()                                                               # read the first 5 lines of the data
print(source_data.info())

## 3. Data Profiling

source_data.dtypes                                                                 # display the data type of each variable

source_data.head()                                                               # display the first 5 rows of source data

source_data.describe(include='all')                                           # displays the statistics

print("Number of rows:" +str(source_data.shape[0]))
print("Number of rows:" +str(source_data.shape[1]))
print("columns :" + "," .join(source_data.columns))

import pandas_profiling                                                      # Get a quick overview for all the variables using pandas_profiling                                         
profile = pandas_profiling.ProfileReport(source_data)
profile.to_file(outputfile="myoutputfile.html") 

_Observations_
<br>
1.The sample set has 85 rows with no null values in it.
<br>
2.The flavor of each candy if it contains chocolate or fruity or caramel or peanut almonds or rice crisp wafers is indicated by 1 or 0.
<br>
3.If it is 1 then it indicates, that the candy has that particular flavor. If it is 0 then that ia absent.
<br>
4.pluribus indicates if it is many candies in a bag or box.
<br>
5.Sugarpercent indicates the the percentile of sugar it falls with in the data set.

plt.figure(figsize=(20,10))
sns.heatmap(source_data.corr().abs(),annot=True)

winners = source_data[source_data.winpercent>source_data.winpercent.quantile(.6)]

fig,axes=plt.subplots(1,9,figsize=(30,10))
for idx,cols in enumerate(list(source_data.columns)[1:10]):
    sns.boxplot(x=cols, y='winpercent', data=source_data, ax=axes[idx],showfliers=True)
plt.show()

_Observations_
<br>
1.From the above plot, it can be observed that 60% people like chocolate candies. So, it is an important parameter.
<br>
2.From the above plot, it can be observed that 60% people like fruity candies. So, it is an important parameter.
<br>
3.From the above plot, it can be observed that 60% people like caramel candies. So, it is an important parameter.
<br>
4.From the above plot, it can be observed that 70% people like peanut/almond candies. So, it is an important parameter.
<br>
5.

### Is there a relationship between sugarpercent and winpercent?

sns.jointplot(x="sugarpercent", y="winpercent", data=winners,kind="kde")

_Observation_
1. From the above plot, it can be observed that the amount of sugar present in the candy does not favour the candy.

### Is there a relationship between pricepercent and winpercent?

sns.jointplot(x="pricepercent", y="winpercent", data=winners,kind="kde")

_Observation-
1. From the above plot, it can be observed that the cost of the candy does not contribute to best favoured candy.

### Most Popular and the least popular candy

popularity = source_data[['competitorname','winpercent']].sort_values(by='winpercent')
pd.concat([popularity.head(5),popularity.tail(5)],axis=0).plot(x='competitorname',y='winpercent',kind='barh',
title='Popularity of various candies',sort_columns=True,figsize = (10,5),legend=False)

_Observation_
1. From the above plot, it can be concluded that Reeses Peanut butter cup is the most popular candy and Nik L Nip is the least popular candy, This has concluded based on the winpercentage.

### Identifying the components that make good candy

from sklearn import tree
reg = tree.DecisionTreeRegressor(max_depth=3).fit(source_data[source_data.columns[1:-1]],source_data[source_data.columns[-1]])
imp = pd.DataFrame.from_dict({'Name':source_data.columns[1:-1],'Importance':reg.feature_importances_})
imp_plt = imp.sort_values(by='Importance',ascending=True).reset_index(drop=True)
imp_plt[imp_plt.Importance>0].plot(kind='barh',x='Name',y='Importance',title='Feature Importance',sort_columns=True,figsize = (10,5),legend=False)

From the above plot, it can be concluded that,chocolate is the component that makes good candy. This has concluded based on the winpercentage


##Model Training

from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
X_train, X_test, y_train, y_test = train_test_split(source_data[source_data.columns[1:-1]],source_data[source_data.columns[-1]], test_size=0.33, random_state=42)
rmse_err = 1[]

from sklearn.tree import DecisionTreeRegressor
reg = DecisionTreeRegressor(max_depth=5).fit(X_train,y_train)
rmse_err.append(math.sqrt(mean_squared_error(y_test,reg.predict(X_test))))
rmse_err[-1]

from sklearn.ensemble import RandomForestRegressor
rf_reg = RandomForestRegressor(n_estimators=200).fit(X_train,y_train)
rmse_err.append(math.sqrt(mean_squared_error(y_test,rf_reg.predict(X_test))))
rmse_err[-1]

from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression

poly = PolynomialFeatures(degree=2)
X_ = poly.fit_transform(X_train)
X_test_ = poly.fit_transform(X_test)

lg = LinearRegression().fit(X_, y_train)

rmse_err.append(math.sqrt(mean_squared_error(y_test,lg.predict(X_test_))))
rmse_err[-1]

from sklearn.linear_model import BayesianRidge
b_reg = BayesianRidge().fit(X_train,y_train)
rmse_err.append(math.sqrt(mean_squared_error(y_test,b_reg.predict(X_test))))
rmse_err[-1]

models = ['Decision Tree','RandomForest','Polynomial','Bayesian Ridge']
pd.DataFrame.from_dict({'Name':models,'RMSE':rmse_err}).sort_values(by='RMSE',ascending=False).plot(x='Name',y='RMSE',kind='barh',sort_columns=True,figsize = (10,5),legend=False,title='Performance of various Regression based algos')